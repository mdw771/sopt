{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.linalg import LinearOperator\n",
    "from typing import Callable, Optional, NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFreeLinearOp(LinearOperator):\n",
    "    def __init__(self, \n",
    "                 operator: Callable[[tf.Tensor], tf.Tensor],\n",
    "                 shape: tf.TensorShape) -> None:\n",
    "        self._operator = operator\n",
    "        self._op_shape = shape\n",
    "        super().__init__(dtype=tf.float32)\n",
    "    \n",
    "    def _matvec(self,\n",
    "                x: tf.Tensor,\n",
    "                adjoint: Optional[bool] = False) -> tf.Tensor:\n",
    "        return self._operator(x)\n",
    "    \n",
    "    def _shape(self) -> tf.TensorShape:\n",
    "        return self._op_shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is adapted from \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/solvers/python/ops/linear_equations.py\n",
    "def conjugate_gradient(operator: LinearOperator,\n",
    "                       rhs: tf.Tensor,\n",
    "                       x: Optional[tf.Tensor] = None,\n",
    "                       preconditioner: Optional[LinearOperator] = None,\n",
    "                       tol: float = 1e-4,\n",
    "                       max_iter: int = 20,\n",
    "                       name: str = \"conjugate_gradient\") -> NamedTuple:\n",
    "    r\"\"\"Conjugate gradient solver.\n",
    "    Solves a linear system of equations `A*x = rhs` for selfadjoint, positive\n",
    "    definite matrix `A` and right-hand side vector `rhs`, using an iterative,\n",
    "    matrix-free algorithm where the action of the matrix A is represented by\n",
    "    `operator`. The iteration terminates when either the number of iterations\n",
    "    exceeds `max_iter` or when the residual norm has been reduced to `tol`\n",
    "    times its initial value, i.e. \\\\(||rhs - A x_k|| <= tol ||rhs||\\\\).\n",
    "    Args:\n",
    "    operator: An object representing a linear operator with attributes:\n",
    "      - shape: Either a list of integers or a 1-D `Tensor` of type `int32` of\n",
    "        length 2. `shape[0]` is the dimension on the domain of the operator,\n",
    "        `shape[1]` is the dimension of the co-domain of the operator. On other\n",
    "        words, if operator represents an N x N matrix A, `shape` must contain\n",
    "        `[N, N]`.\n",
    "      - dtype: The datatype of input to and output from `apply`.\n",
    "      - apply: Callable object taking a vector `x` as input and returning a\n",
    "        vector with the result of applying the operator to `x`, i.e. if\n",
    "       `operator` represents matrix `A`, `apply` should return `A * x`.\n",
    "    rhs: A rank-1 `Tensor` of shape `[N]` containing the right-hand size vector.\n",
    "    preconditioner: An object representing a linear operator, see `operator`\n",
    "      for detail. The preconditioner should approximate the inverse of `A`.\n",
    "      An efficient preconditioner could dramatically improve the rate of\n",
    "      convergence. If `preconditioner` represents matrix `M`(`M` approximates\n",
    "      `A^{-1}`), the algorithm uses `preconditioner.apply(x)` to estimate\n",
    "      `A^{-1}x`. For this to be useful, the cost of applying `M` should be\n",
    "      much lower than computing `A^{-1}` directly.\n",
    "    x: A rank-1 `Tensor` of shape `[N]` containing the initial guess for the\n",
    "      solution.\n",
    "    tol: A float scalar convergence tolerance.\n",
    "    max_iter: An integer giving the maximum number of iterations.\n",
    "    name: A name scope for the operation.\n",
    "    Returns:\n",
    "    output: A namedtuple representing the final state with fields:\n",
    "      - i: A scalar `int32` `Tensor`. Number of iterations executed.\n",
    "      - x: A rank-1 `Tensor` of shape `[N]` containing the computed solution.\n",
    "      - r: A rank-1 `Tensor` of shape `[M]` containing the residual vector.\n",
    "      - p: A rank-1 `Tensor` of shape `[N]`. `A`-conjugate basis vector.\n",
    "      - gamma: \\\\(r \\dot M \\dot r\\\\), equivalent to  \\\\(||r||_2^2\\\\) when\n",
    "        `preconditioner=None`.\n",
    "    \"\"\"\n",
    "    # ephemeral class holding CG state.\n",
    "    class CGState(NamedTuple):\n",
    "        i: tf.Tensor\n",
    "        x: tf.Tensor\n",
    "        r: tf.Tensor\n",
    "        p: tf.Tensor\n",
    "        gamma: tf.Tensor\n",
    "    \n",
    "    def stopping_criterion(state):\n",
    "        with tf.name_scope('cg_cond'):\n",
    "            output = tf.linalg.norm(state.r) > tol\n",
    "        return output\n",
    "\n",
    "    def cg_step(state):  # pylint: disable=missing-docstring\n",
    "        with tf.name_scope('cg_body'):\n",
    "            z = operator.matvec(state.p)\n",
    "            alpha = state.gamma / tf.tensordot(state.p, z, 1)\n",
    "            x = state.x + alpha * state.p\n",
    "            r = state.r - alpha * z\n",
    "            if preconditioner is None:\n",
    "                gamma = tf.tensordot(r,r,1)\n",
    "                beta = gamma / state.gamma\n",
    "                p = r + beta * state.p\n",
    "            else:\n",
    "                q = preconditioner.matvec(r)\n",
    "                gamma = tf.tensordot(r,q,1)\n",
    "                beta = gamma / state.gamma\n",
    "                p = q + beta * state.p\n",
    "            output = CGState(i=state.i + 1, x=x, r=r, p=p, gamma=gamma)\n",
    "        return output\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        if x is None:\n",
    "            x = tf.zeros_like(rhs)\n",
    "            r0 = rhs\n",
    "        else:\n",
    "            r0 = rhs - operator.matvec(x)\n",
    "        if preconditioner is None:\n",
    "            p0 = r0\n",
    "        else:\n",
    "            p0 = preconditioner.matvec(r0)\n",
    "        gamma0 = tf.tensordot(r0, p0, 1)\n",
    "        tol *= tf.linalg.norm(r0)\n",
    "        state = CGState(i=tf.constant(0, dtype=tf.int32), x=x, r=r0, p=p0, gamma=gamma0)\n",
    "        state = tf.while_loop(stopping_criterion, cg_step, \n",
    "                              [state], maximum_iterations=max_iter,\n",
    "                              back_prop=False,\n",
    "                              name='cg_while')\n",
    "        return CGState(\n",
    "            state.i,\n",
    "            x=tf.squeeze(state.x),\n",
    "            r=tf.squeeze(state.r),\n",
    "            p=tf.squeeze(state.p),\n",
    "            gamma=state.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cg_solve(linear_op, b, x_init, tol=1e-7, maxiter=None):\n",
    "    \"\"\"\n",
    "    This is functionally identical to the conjugate_gradient method,\n",
    "    except without the option to use the preconditioner.\n",
    "    \n",
    "    Source:\n",
    "    https://stanford.edu/~boyd/papers/pdf/cvxflow_pyhpc.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(linear_op, tf.Tensor):\n",
    "        matvec_op = lambda x: tf.matmul(linear_op, x)\n",
    "    elif isinstance(linear_op, tf.linalg.LinearOperator):\n",
    "        matvec_op = linear_op.matvec\n",
    "    else:\n",
    "        matvec_op = linear_op\n",
    "    \n",
    "    delta = tol * tf.norm(b)\n",
    "\n",
    "    def body(x, k, r_norm_sq, r, p):\n",
    "        Ap = matvec_op(p)#A(p)\n",
    "        alpha = r_norm_sq / tf.tensordot(p, Ap, 1)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        r_norm_sq_prev = r_norm_sq\n",
    "        r_norm_sq = tf.tensordot(r,r,1)\n",
    "        beta = r_norm_sq / r_norm_sq_prev\n",
    "        p = r + beta * p\n",
    "        return (x, k + 1, r_norm_sq, r, p)\n",
    "\n",
    "    def cond(x, k, r_norm_sq, r, p):\n",
    "        return tf.sqrt(r_norm_sq) > delta\n",
    "\n",
    "    r = b - matvec_op(x_init)\n",
    "    loop_vars = (x_init, tf.constant(0), tf.tensordot(r, r, 1), r, r)\n",
    "    return tf.while_loop(cond, body, loop_vars, maximum_iterations=maxiter, back_prop=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be buggy\n",
    "def linear_cg_solve_martens(linear_op, b,\n",
    "                            x_init, \n",
    "                            tol=5e-4, \n",
    "                            maxiter=100, \n",
    "                            miniter=10):\n",
    "    \"\"\"Source:\n",
    "    https://stanford.edu/~boyd/papers/pdf/cvxflow_pyhpc.pdf\n",
    "    adapted for the algorithm in\n",
    "    \"Deep learning via Hessian-free optimization\"\n",
    "    by J. Martens\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(linear_op, tf.Tensor):\n",
    "        matvec_op = lambda x: tf.matmul(linear_op, x)\n",
    "    elif isinstance(linear_op, tf.linalg.LinearOperator):\n",
    "        matvec_op = linear_op.matvec\n",
    "    else:\n",
    "        matvec_op = linear_op\n",
    "    \n",
    "    quadratic_vals = tf.TensorArray(tf.float32, \n",
    "                                    size=1, \n",
    "                                    element_shape=[], \n",
    "                                    clear_after_read=False,\n",
    "                                    dynamic_size=True)\n",
    "    \n",
    "    def quadratic(x):\n",
    "        Ax = matvec_op(x)\n",
    "        xAx = tf.tensordot(x, Ax, 1)\n",
    "        return 0.5 * xAx - tf.tensordot(x, b, 1)\n",
    "\n",
    "    def body(x, k, quadratic_vals, r_norm_sq, r, p):\n",
    "        Ap = matvec_op(p)#A(p)\n",
    "        pAp = tf.tensordot(p, Ap, 1)\n",
    "        alpha = r_norm_sq / pAp\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        r_norm_sq_prev = r_norm_sq\n",
    "        r_norm_sq = tf.tensordot(r,r,1)\n",
    "        beta = r_norm_sq / r_norm_sq_prev\n",
    "        p = r + beta * p\n",
    "        return (x, k+1, quadratic_vals.write(k, quadratic(x)), r_norm_sq, r, p)\n",
    "\n",
    "    def cond(x, k, quadratic_vals, r_norm_sq, r, p):\n",
    "        kmax = tf.maximum(miniter, tf.cast(0.1 * tf.cast(k-1, 'float32'), 'int32'))\n",
    "        minindx = tf.maximum(0, k-1 - kmax)\n",
    "        cond1 = tf.greater(k-1, kmax)\n",
    "        cond2 = tf.less(quadratic_vals.read(k-1), 0.)\n",
    "        delta = tf.cast(kmax, 'float32') * tol\n",
    "        cond3 = tf.less((quadratic_vals.read(k-1) - quadratic_vals.read(minindx)) / quadratic_vals.read(k-1), delta)\n",
    "        cond4 = tf.logical_not(tf.logical_and(cond1, tf.logical_and(cond2, cond3)))\n",
    "        return cond4\n",
    "        \n",
    "    r = b - matvec_op(x_init)\n",
    "    loop_vars = (x_init, tf.constant(0, dtype='int32'), quadratic_vals, tf.tensordot(r, r, 1), r, r)\n",
    "    \n",
    "    x_new, k_new, quadratic_vals, r_norm_sq, r, p = body(*loop_vars)\n",
    "    loop_vars = (x_new, k_new, quadratic_vals, r_norm_sq, r, p)\n",
    "    return tf.while_loop(cond, body, loop_vars, maximum_iterations=maxiter, back_prop=False)[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
