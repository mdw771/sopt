{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.gradients_impl import _hessian_vector_product\n",
    "from typing import Callable, List\n",
    "from sopt.optimizers.tensorflow.utils import MatrixFreeLinearOp, conjugate_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMA(object):\n",
    "    def __init__(self, \n",
    "                 input_var: tf.Variable, \n",
    "                 predictions_fn: Callable[[tf.Tensor], tf.Tensor], \n",
    "                 loss_fn: Callable[[tf.Tensor], tf.Tensor], \n",
    "                 name: str,\n",
    "                 damping_factor: float = 1.0, \n",
    "                 damping_update_factor: float = 2/3,\n",
    "                 update_cond_threshold_low: float = 0.25, \n",
    "                 update_cond_threshold_high: float = 0.75,\n",
    "                 damping_threshold_low: float = 1e-7,\n",
    "                 damping_threshold_high: float = 1e7,\n",
    "                 max_cg_iter: int = 20,\n",
    "                 cg_tol: float = 1e-5, \n",
    "                 xtol: float = 1e-6,\n",
    "                 ftol: float = 1e-6,\n",
    "                 gtol: float = 1e-6,\n",
    "                 squared_loss: bool = True) -> None:\n",
    "        \n",
    "        self._name = name\n",
    "        self._input_var = input_var\n",
    "        \n",
    "        self._predictions_fn = predictions_fn\n",
    "        self._loss_fn = loss_fn\n",
    "        \n",
    "        self._predictions_fn_tensor = self._predictions_fn(self._input_var)\n",
    "        self._loss_fn_tensor = self._loss_fn(self._predictions_fn_tensor)\n",
    "        \n",
    "        # Multiplicating factor to update the damping factor at the end of each cycle\n",
    "        self._damping_update_factor = damping_update_factor\n",
    "        self._update_cond_threshold_low = update_cond_threshold_low\n",
    "        self._update_cond_threshold_high =  update_cond_threshold_high\n",
    "        self._damping_threshold_low = damping_threshold_low\n",
    "        self._damping_threshold_high = damping_threshold_high\n",
    "        self._max_cg_iter = max_cg_iter\n",
    "        \n",
    "        self._cg_tol = cg_tol\n",
    "        self._xtol = xtol\n",
    "        self._ftol = ftol\n",
    "        self._gtol = gtol\n",
    "        \n",
    "        self._squared_loss = squared_loss\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self._damping_factor = tf.get_variable(\"lambda\", dtype=tf.float32, \n",
    "                                                   initializer=damping_factor,\n",
    "                                                   trainable=False)\n",
    "            self._update_var = tf.get_variable(\"delta\", dtype=tf.float32,\n",
    "                                               initializer=tf.ones_like(self._input_var),\n",
    "                                               trainable=False)\n",
    "            self._dummy_var = tf.get_variable(\"dummy\", dtype=tf.float32, \n",
    "                                              initializer=tf.zeros_like(self._predictions_fn_tensor),\n",
    "                                              trainable=False)\n",
    "            \n",
    "            self._loss_before_update = tf.get_variable(\"loss_before_update\", dtype=tf.float32,\n",
    "                                                     initializer=0.,\n",
    "                                                     trainable=False)\n",
    "            self._expected_quadratic_change = tf.get_variable(\"expected_quadratic_change\", \n",
    "                                                         dtype=tf.float32,\n",
    "                                                         initializer=0.,\n",
    "                                                         trainable=False)\n",
    "            self._iteration = tf.get_variable(\"iteration\", shape=[], dtype=tf.int32,\n",
    "                                              initializer=tf.zeros_initializer,\n",
    "                                              trainable=False)\n",
    "            \n",
    "            self._total_cg_iterations = tf.get_variable(\"total_cg_iterations\", \n",
    "                                                        dtype=tf.int32, shape=[],\n",
    "                                                        initializer=tf.zeros_initializer,\n",
    "                                                        trainable=False)\n",
    "        # Set up the second order calculations to define matrix-free linear ops.\n",
    "        self._setup_second_order()\n",
    "    \n",
    "    def _setup_hessian_vector_product(self, \n",
    "                                      jvp_fn: Callable[[tf.Tensor], tf.Tensor],\n",
    "                                      x: tf.Tensor,\n",
    "                                      v_constant: tf.Tensor) -> tf.Tensor:\n",
    "        predictions_this = self._predictions_fn(v_constant)\n",
    "        loss_this = self._loss_fn(predictions_this)\n",
    "        hjvp = _hessian_vector_product(ys=[loss_this],\n",
    "                                       xs=[predictions_this],\n",
    "                                       v=[jvp_fn(x)])\n",
    "        jhjvp = tf.gradients(predictions_this, v_constant, hjvp)[0]\n",
    "        return jhjvp\n",
    "        \n",
    "    def _setup_second_order(self) -> None:\n",
    "        with tf.name_scope(self._name + '_gngvp'):\n",
    "            vjp = tf.gradients(self._predictions_fn_tensor, self._input_var, self._dummy_var,\n",
    "                                stop_gradients=[self._dummy_var],\n",
    "                                name='vjp')[0]\n",
    "            \n",
    "            jvp_fn = lambda x: tf.gradients(vjp, self._dummy_var, x, name='jvpz')[0]\n",
    "            self.vjp = vjp\n",
    "            self.jvp_fn = jvp_fn\n",
    "            \n",
    "            if self._squared_loss:\n",
    "                hjvp_fn = jvp_fn\n",
    "                # Ignore the v input\n",
    "                self._jhjvp_fn = lambda x, v_constant: tf.gradients(self._predictions_fn_tensor, \n",
    "                                                     self._input_var,\n",
    "                                                     hjvp_fn(x))[0]\n",
    "            else:\n",
    "                self._jhjvp_fn = lambda x, v_constant: self._setup_hessian_vector_product(jvp_fn, x, \n",
    "                                                                                    v_constant)\n",
    "            \n",
    "            self._grads = tf.gradients(self._loss_fn_tensor, self._input_var)[0]\n",
    "            \n",
    "    \n",
    "    def minimize(self) -> tf.Operation:\n",
    "        tf.logging.warning(\"The ftol, gtol, and xtol conditions are adapted from \"\n",
    "                           + \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html.\"\n",
    "                           + \"This is a test version, and there is no guarantee that these work as intended.\")\n",
    "        with tf.name_scope(self._name + '_minimize_step'):\n",
    "            \n",
    "            grads_norm = tf.norm(self._grads, ord=np.inf)\n",
    "            xtol_norm = self._xtol * (self._xtol + tf.norm(self._input_var, ord=2))\n",
    "            update_norm = tf.norm(self._update_var, ord=2)\n",
    "            \n",
    "            assert_gtol_op = tf.assert_greater(grads_norm, self._gtol, \n",
    "                                               message='Gradient norm lower than tolerance.')\n",
    "            assert_xtol_op = tf.assert_greater(update_norm, xtol_norm,\n",
    "                                               message='Damping factor lower than ')\n",
    "            \n",
    "            \n",
    "            with tf.control_dependencies([assert_gtol_op, assert_xtol_op]):\n",
    "                store_loss_op = tf.assign(self._loss_before_update, self._loss_fn_tensor,\n",
    "                                          name='store_loss_op')\n",
    "            jhjvp_fn_l_h = lambda l, h, v_constant: self._jhjvp_fn(h, v_constant) + l * h\n",
    "            linear_b = -self._grads\n",
    "                \n",
    "            def _body(damping, update, reduction_ratio, loss_new, cg_iterations, v_constant):\n",
    "                linear_ax = MatrixFreeLinearOp(lambda h: jhjvp_fn_l_h(damping, h, v_constant),\n",
    "                                               tf.TensorShape((self._input_var.shape.dims[0],\n",
    "                                                               self._input_var.shape.dims[0])))\n",
    "                cg_solve = conjugate_gradient(operator=linear_ax, \n",
    "                                              rhs=linear_b, \n",
    "                                              x=tf.zeros_like(self._update_var),#self._update_var,\n",
    "                                              tol=self._cg_tol,\n",
    "                                              max_iter=self._max_cg_iter)\n",
    "                update = tf.identity(cg_solve.x, name='cg_solved')\n",
    "                expected_quadratic_change = -0.5 * tf.tensordot(update, damping * update + linear_b, 1)\n",
    "                optimized_var = self._input_var + update\n",
    "                loss_new = self._loss_fn(self._predictions_fn(optimized_var))\n",
    "                loss_diff = loss_new - self._loss_before_update\n",
    "                \n",
    "                ftol_factor = tf.abs(self._ftol * self._loss_before_update)\n",
    "                assert_ftol_op = tf.assert_greater(tf.abs(loss_diff), ftol_factor,\n",
    "                                                   message='Function update norm lower than...')\n",
    "                #ftol_cond = tf.logical_or(tf.math.greater(tf.abs(loss_new - self._loss_before_update), \n",
    "                #                                          ftol_factor),\n",
    "                #                          tf.math.greater(tf.abs(expected_quadratic_change), ftol_factor))\n",
    "                # \n",
    "                #assert_ftol_op = tf.Assert(ftol_cond, [ftol_factor])\n",
    "                \n",
    "                with tf.control_dependencies([assert_ftol_op]):\n",
    "                    reduction_ratio = loss_diff / expected_quadratic_change\n",
    "                    #reduction_ratio = (loss_new / self._loss_before_update - 1.) / \n",
    "                    #(expected_quadratic_change / self._loss_before_update)\n",
    "                \n",
    "                f1 = lambda: tf.constant(1.0 / self._damping_update_factor)\n",
    "                f2 = lambda: tf.constant(self._damping_update_factor)\n",
    "                f3 = lambda: tf.constant(1.0)\n",
    "\n",
    "                update_factor = tf.case({tf.less(reduction_ratio, self._update_cond_threshold_low):f1, \n",
    "                                 tf.greater(reduction_ratio, self._update_cond_threshold_high):f2},\n",
    "                                 default=f3, exclusive=True)\n",
    "                \n",
    "                damping_new = damping * update_factor\n",
    "\n",
    "                damping_new = tf.clip_by_value(damping * update_factor, \n",
    "                                               self._damping_threshold_low, \n",
    "                                               self._damping_threshold_high)\n",
    "                return (damping_new, update, reduction_ratio, loss_new, cg_iterations + cg_solve.i, v_constant)\n",
    "            \n",
    "            def _cond(damping, update, reduction_ratio, loss_new, cg_iterations, v_constant):\n",
    "                return tf.math.logical_and(reduction_ratio <= 0, \n",
    "                                          self._loss_before_update > 10 * np.finfo('float32').eps)\n",
    "            \n",
    "            with tf.control_dependencies([store_loss_op]):\n",
    "                damping_new, update, reduction_ratio, loss_new, cg_iterations, _ = tf.while_loop(_cond, _body,\n",
    "                                                                                       (self._damping_factor, \n",
    "                                                                                        self._update_var, 0., 0., \n",
    "                                                                                        tf.constant(0, dtype=tf.int32),\n",
    "                                                                                        self._input_var), \n",
    "                                                                                       back_prop=False)\n",
    "            \n",
    "            update_ops = [tf.assign(self._damping_factor, damping_new),\n",
    "                              tf.assign(self._update_var, update),\n",
    "                              tf.assign(self._input_var, self._input_var + update)]\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                cg_counter_op = tf.assign(self._total_cg_iterations, self._total_cg_iterations + cg_iterations,\n",
    "                                          name='cg_counter_op')\n",
    "            with tf.control_dependencies([cg_counter_op]):\n",
    "                counter_op = tf.assign(self._iteration, self._iteration + 1, name='counter_op')\n",
    "        return counter_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
