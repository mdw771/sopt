{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import autograd as ag\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN CONSTRUCTION\n",
    "class Curveball(object):\n",
    "    \"\"\"Adapted from:\n",
    "    https://github.com/jotaf98/curveball\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_variable: np.ndarray,\n",
    "                 predictions_fn: Callable[[np.ndarray], np.ndarray],\n",
    "                 loss_fn: Callable[[np.ndarray], float],\n",
    "                 damping_factor: float = 1.0, \n",
    "                 damping_update_factor: float = 0.999,\n",
    "                 damping_update_frequency: int = 5,\n",
    "                 update_cond_threshold_low: float = 0.5, \n",
    "                 update_cond_threshold_high: float = 1.5,\n",
    "                 damping_threshold_low: float = 1e-7,\n",
    "                 damping_threshold_high: float = 1e7,\n",
    "                 alpha_init: float = 1.0,\n",
    "                 squared_loss: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Paramters:\n",
    "        input_var: \n",
    "        1-d numpy array. Flatten before passing, if necessary.\n",
    "        \n",
    "        loss_input_fn: \n",
    "        Function that takes in input_var as the only input parameter.\n",
    "        This should output a 1-d array, that is then passed to loss_fn for the \n",
    "        actual loss calculation.  \n",
    "        Separating the loss calculation into a two step process this way \n",
    "        simplifies the second order calculations.\n",
    "        \n",
    "        loss_fn:\n",
    "        Function that takes in the output of loss_input_fn and \n",
    "        calculates the singular loss value.\n",
    "        \n",
    "        from \n",
    "        The alpha should generally just be 1.0 and doesn't change. \n",
    "        The beta and rho values are updated at each cycle, so there is no intial value.\"\"\"\n",
    "        \n",
    "        self._input_var = input_variable\n",
    "        self._predictions_fn = predictions_fn\n",
    "        self._loss_fn = loss_fn\n",
    "        \n",
    "        # Multiplicating factor to update the damping factor at the end of each cycle\n",
    "        self._damping_factor = damping_factor\n",
    "        self._damping_update_factor = damping_update_factor\n",
    "        self._damping_update_frequency = damping_update_frequency\n",
    "        self._update_cond_threshold_low = update_cond_threshold_low\n",
    "        self._update_cond_threshold_high =  update_cond_threshold_high\n",
    "        self._damping_threshold_low = damping_threshold_low\n",
    "        self._damping_threshold_high = damping_threshold_high\n",
    "        self._squared_loss = squared_loss\n",
    "        self._alpha = alpha_init\n",
    "        \n",
    "        self._z = np.zeros_like(self._input_var)\n",
    "        self._iteration = 0\n",
    "        \n",
    "        self._vjp = ag.make_vjp(self._predictions_fn)\n",
    "        self._jvp = ag.differential_operators.make_jvp_reversemode(self._predictions_fn)\n",
    "        \n",
    "        self._grad = ag.grad(self._loss_fn)\n",
    "        \n",
    "        if self._squared_loss:\n",
    "            self._hjvp = self._jvp\n",
    "        else:\n",
    "            self._hjvp = ag.differential_operators.make_hvp(self._loss_fn)\n",
    "        \n",
    "        self._iteration = 0\n",
    "\n",
    "\n",
    "    def _matrix_vector_updates(self) -> List[float]:   \n",
    "        vjp_fun_this, predictions_array = self._vjp(self._input_var) \n",
    "        jvp_fun_this = self._jvp(self._input_var)\n",
    "        \n",
    "        loss_before_update = self._loss_fn(predictions_array)\n",
    "        \n",
    "        if self._squared_loss: \n",
    "            hjvp_fun_this = lambda x: x#jvp_fun_this\n",
    "            jloss = self._grad(predictions_array)\n",
    "        else:\n",
    "            hjvp_fun_this, jloss = self._hjvp(predictions_array)\n",
    "        \n",
    "        jvpz = jvp_fun_this(self._z)\n",
    "        hjvpz = hjvp_fun_this(jvpz)\n",
    "        \n",
    "        jhjvpz = vjp_fun_this(hjvpz + jloss)\n",
    "        \n",
    "        deltaz = jhjvpz + self._damping_factor * self._z  \n",
    "        jvpdz = jvp_fun_this(deltaz)\n",
    "        \n",
    "        if self._squared_loss:\n",
    "            hjvpdz = jvpdz\n",
    "        else:\n",
    "            hjvpdz = hjvp_fun_this(jvpdz)\n",
    "            \n",
    "        a11 = np.sum(hjvpdz * jvpdz)\n",
    "        a12 = np.sum(jvpz * hjvpdz)\n",
    "        a22 = np.sum(jvpz * hjvpz)\n",
    "        \n",
    "        b1 = np.sum(jloss * jvpdz)\n",
    "        b2 = np.sum(jloss * jvpz)\n",
    "        \n",
    "        a11 = a11 + np.sum(deltaz * deltaz) * self._damping_factor\n",
    "        a12 = a12 + np.sum(deltaz * self._z) * self._damping_factor\n",
    "        a22 = a22 + np.sum(self._z * self._z) * self._damping_factor\n",
    "        \n",
    "        A = np.array([[a11, a12],[a12, a22]])\n",
    "        b = np.array([b1, b2])\n",
    "        \n",
    "        m_b = np.linalg.pinv(A) @ b\n",
    "        \n",
    "        beta = m_b[0]\n",
    "        rho = -m_b[1]\n",
    "        \n",
    "        M = -0.5 * m_b @ b\n",
    "        \n",
    "        self._z = rho * self._z - beta * deltaz\n",
    "        self._input_var = self._input_var + self._alpha * self._z\n",
    "        return loss_before_update, M\n",
    "    \n",
    "    def _damping_update(self, \n",
    "                        loss_before_update: float,\n",
    "                        expected_quadratic_change: float) -> None:\n",
    "        \n",
    "        loss_after_update = self._loss_fn(self._predictions_fn(self._input_var))\n",
    "        actual_loss_change = loss_after_update - loss_before_update\n",
    "        gamma = actual_loss_change / expected_quadratic_change\n",
    "        \n",
    "        if gamma < self._update_cond_threshold_low:\n",
    "            self._damping_factor = self._damping_factor / self._damping_update_factor\n",
    "        elif gamma > self._update_cond_threshold_high:\n",
    "            self._damping_factor = self._damping_factor * self._damping_update_factor\n",
    "        \n",
    "        self._damping_factor = np.clip(self._damping_factor, \n",
    "                                       a_min=self._damping_threshold_low, \n",
    "                                       a_max=self._damping_threshold_high)\n",
    "    \n",
    "    def minimize(self) -> np.ndarray:\n",
    "        loss_before_update, M = self._matrix_vector_updates()\n",
    "        \n",
    "        self._expected_quadratic_change = M\n",
    "        \n",
    "        if self._iteration % self._damping_update_frequency == 0:\n",
    "            self._damping_update(loss_before_update, M)\n",
    "        self._iteration += 1       \n",
    "        return self._input_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
