{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN CONSTRUCTION\n",
    "class LMA(object):\n",
    "    def __init__(self, input_var, loss_fn_input, loss_fn, squared_loss=True):\n",
    "        \n",
    "        self._input_var = input_var\n",
    "        self._loss_fn_input = loss_fn_input\n",
    "        self._loss_fn = loss_fn\n",
    "        #self.damping_factor = damping_factor\n",
    "        self._squared_loss = squared_loss\n",
    "        \n",
    "        self._update_var = tf.Variable(tf.zeros_like(input_var))\n",
    "        self._dummy_var = tf.Variable(tf.zeros_like(loss_fn_input))\n",
    "        \n",
    "        self._vjp = tf.gradients(loss_fn_input, input_var, self._dummy_var)[0]\n",
    "        self._grads = tf.gradients(loss_fn, input_var)[0]\n",
    "        \n",
    "        #self.linear_op = type('MyLinearOperator', (tf.linalg.LinearOperator),\n",
    "        #                      {\"_shape\": lambda self: tf.TensorShape([input_var.shape[0],\n",
    "        #                                                              input_var.shape[0]]),\n",
    "        #                       \"_matvec\": lambda self, x: self.matrix_free_linear_op(x)})\n",
    "        self._reduction_ratio_denom = tf.Variable(0., trainable=False)\n",
    "    \n",
    "    def _matrix_free_linear_op(self, x, damping_factor):\n",
    "        jvp = tf.gradients(self._vjp, self._dummy_var, x)[0]\n",
    "        \n",
    "        # this could be off by a factor of two\n",
    "        if self._squared_loss:\n",
    "            hjvp = jvp\n",
    "        else:\n",
    "            hjvp = tf.gradients(tf.gradients(self._loss_fn, self._loss_fn_input)[0][None, :] @ jvp[:,None], \n",
    "                                self._loss_fn_input, stop_gradients=jvp)[0]\n",
    "        \n",
    "        jhjvp = tf.gradients(self._loss_fn_input, self._input_var, hjvp)[0]\n",
    "        jhjvp = jhjvp + damping_factor * x\n",
    "        return jhjvp \n",
    "    \n",
    "    def update(self, damping_factor=0.0, tol=5e-4, maxiter=100):\n",
    "        linear_op = lambda x: self._matrix_free_linear_op(x, damping_factor)\n",
    "        # Use the prescription in the Marten paper and initialize the cg procedure \n",
    "        # with output from previous iteration instead of zeros\n",
    "        \n",
    "        # quadratic approximation to the loss function \n",
    "        # this is for the reduction ratio calculation for the lambda update\n",
    "        # follows the recipe in the Marten paper\n",
    "        \n",
    "        update_temp, _, quadratic_vals = linear_cg_solve_martens(linear_op, -self._grads, self._update_var, tol, maxiter)\n",
    "        quadratic_final = quadratic_vals.stack()[-1]\n",
    "        update_ops =  [self._update_var.assign(update_temp), \n",
    "                      self._reduction_ratio_denom.assign(quadratic_final)]#0.5 * tf.tensordot(update_temp, linear_op(update_temp),1) \n",
    "                                                         #+ tf.tensordot(update_temp, self._grads,1))]\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            assign_op = self._input_var.assign_add(self._update_var)\n",
    "            \n",
    "        return assign_op\n",
    "    \n",
    "    def revert(self):\n",
    "        assign_op = self._input_var.assign_sub(self._update_var)\n",
    "        return assign_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cg_solve(linear_op, b, x_init, tol=1e-7, maxiter=None):\n",
    "    \"\"\"Source:\n",
    "    https://stanford.edu/~boyd/papers/pdf/cvxflow_pyhpc.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(linear_op, tf.Tensor):\n",
    "        matvec_op = lambda x: tf.matmul(linear_op, x)\n",
    "    elif isinstance(linear_op, tf.linalg.LinearOperator):\n",
    "        matvec_op = linear_op.matvec\n",
    "    else:\n",
    "        matvec_op = linear_op\n",
    "    \n",
    "    delta = tol * tf.norm(b)\n",
    "\n",
    "    def body(x, k, r_norm_sq, r, p):\n",
    "        Ap = matvec_op(p)#A(p)\n",
    "        alpha = r_norm_sq / tf.tensordot(p, Ap, 1)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        r_norm_sq_prev = r_norm_sq\n",
    "        r_norm_sq = tf.tensordot(r,r,1)\n",
    "        beta = r_norm_sq / r_norm_sq_prev\n",
    "        p = r + beta * p\n",
    "        return (x, k + 1, r_norm_sq, r, p)\n",
    "\n",
    "    def cond(x, k, r_norm_sq, r, p):\n",
    "        return tf.sqrt(r_norm_sq) > delta\n",
    "\n",
    "    r = b - matvec_op(x_init)\n",
    "    loop_vars = (x_init, tf.constant(0), tf.tensordot(r, r, 1), r, r)\n",
    "    return tf.while_loop(cond, body, loop_vars, maximum_iterations=maxiter, back_prop=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cg_solve_martens(linear_op, b,\n",
    "                            x_init, \n",
    "                            tol=5e-4, \n",
    "                            maxiter=100, \n",
    "                            miniter=10):\n",
    "    \"\"\"Source:\n",
    "    https://stanford.edu/~boyd/papers/pdf/cvxflow_pyhpc.pdf\n",
    "    adapted for the algorithm in\n",
    "    \"Deep learning via Hessian-free optimization\"\n",
    "    by J. Martens\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(linear_op, tf.Tensor):\n",
    "        matvec_op = lambda x: tf.matmul(linear_op, x)\n",
    "    elif isinstance(linear_op, tf.linalg.LinearOperator):\n",
    "        matvec_op = linear_op.matvec\n",
    "    else:\n",
    "        matvec_op = linear_op\n",
    "    \n",
    "    quadratic_vals = tf.TensorArray(tf.float32, \n",
    "                                    size=1, \n",
    "                                    element_shape=[], \n",
    "                                    clear_after_read=False,\n",
    "                                    dynamic_size=True)\n",
    "    \n",
    "    def quadratic(x):\n",
    "        Ax = matvec_op(x)\n",
    "        xAx = tf.tensordot(x, Ax, 1)\n",
    "        return 0.5 * xAx - tf.tensordot(x, b, 1)\n",
    "\n",
    "    def body(x, k, quadratic_vals, r_norm_sq, r, p):\n",
    "        Ap = matvec_op(p)#A(p)\n",
    "        pAp = tf.tensordot(p, Ap, 1)\n",
    "        alpha = r_norm_sq / pAp\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        r_norm_sq_prev = r_norm_sq\n",
    "        r_norm_sq = tf.tensordot(r,r,1)\n",
    "        beta = r_norm_sq / r_norm_sq_prev\n",
    "        p = r + beta * p\n",
    "        return (x, k+1, quadratic_vals.write(k, quadratic(x)), r_norm_sq, r, p)\n",
    "\n",
    "    def cond(x, k, quadratic_vals, r_norm_sq, r, p):\n",
    "        kmax = tf.maximum(miniter, tf.cast(0.1 * tf.cast(k-1, 'float32'), 'int32'))\n",
    "        minindx = tf.maximum(0, k-1 - kmax)\n",
    "        cond1 = tf.greater(k-1, kmax)\n",
    "        cond2 = tf.less(quadratic_vals.read(k-1), 0.)\n",
    "        delta = tf.cast(kmax, 'float32') * tol\n",
    "        cond3 = tf.less((quadratic_vals.read(k-1) - quadratic_vals.read(minindx)) / quadratic_vals.read(k-1), delta)\n",
    "        cond4 = tf.logical_not(tf.logical_and(cond1, tf.logical_and(cond2, cond3)))\n",
    "        return cond4\n",
    "        \n",
    "    r = b - matvec_op(x_init)\n",
    "    loop_vars = (x_init, tf.constant(0, dtype='int32'), quadratic_vals, tf.tensordot(r, r, 1), r, r)\n",
    "    \n",
    "    x_new, k_new, quadratic_vals, r_norm_sq, r, p = body(*loop_vars)\n",
    "    loop_vars = (x_new, k_new, quadratic_vals, r_norm_sq, r, p)\n",
    "    return tf.while_loop(cond, body, loop_vars, maximum_iterations=maxiter, back_prop=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fit the run conditions inside the LMA class\n",
    "class tfLMAPhaseRetriever(tfPhaseRetriever):\n",
    "    \n",
    "    def __init__(self, *args, \n",
    "                 max_cg_iter_per_step=100,\n",
    "                 lambda_init=1,\n",
    "                 lambda_damping=[2,3],\n",
    "                 **kwargs,):\n",
    "        self.max_cg_iter_per_step = max_cg_iter_per_step\n",
    "        self.obj_lambda = lambda_init\n",
    "        self.probe_lambda = lambda_init\n",
    "        self.lambda_damping = lambda_damping\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    \n",
    "    def initLossAndOptimizers(self):\n",
    "        self.lossFunction()\n",
    "        self.obj_lma = LMA(self.tf_obj, self.batch_differences, self.loss_fn)\n",
    "        self.obj_lambda_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "        self.obj_update_op = self.obj_lma.update(self.obj_lambda_placeholder,\n",
    "                                                 maxiter=self.max_cg_iter_per_step)\n",
    "        self.obj_revert_op = self.obj_lma.revert()\n",
    "        \n",
    "        self.probe_lma = LMA(self.tf_probe, self.batch_differences, self.loss_fn)\n",
    "        self.probe_lambda_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "        self.probe_update_op = self.probe_lma.update(self.probe_lambda_placeholder,\n",
    "                                                     maxiter=self.max_cg_iter_per_step)\n",
    "        self.probe_revert_op = self.probe_lma.revert()\n",
    "\n",
    "    def run(self, n_iterations: int, \n",
    "            n_probe_fixed_iterations: int=0,\n",
    "            obj_clip: bool=False,\n",
    "            disable_progress_bar: bool=False):\n",
    "        \n",
    "        \n",
    "        self.session.run(self.assign_op)\n",
    "        lossval_prev = self.session.run(self.loss_fn)\n",
    "        for i in tqdm(range(n_iterations), disable=disable_progress_bar):\n",
    "            self.session.run(self.assign_op)\n",
    "            \n",
    "            while True:\n",
    "                self.session.run(self.obj_update_op, \n",
    "                                 feed_dict={self.obj_lambda_placeholder:self.obj_lambda})\n",
    "                lossval_new, reduction_ratio_denom = self.session.run([self.loss_fn, \n",
    "                                                                       self.obj_lma._reduction_ratio_denom])\n",
    "                # The reduction ratio framework is based on the per christian hansen book\n",
    "                reduction_ratio = (lossval_new - lossval_prev) / reduction_ratio_denom \n",
    "                \n",
    "                if reduction_ratio < 0.25:\n",
    "                    self.obj_lambda *= self.lambda_damping[0]\n",
    "                    \n",
    "                elif reduction_ratio > 0.75:\n",
    "                    self.obj_lambda /= self.lambda_damping[1]\n",
    "                \n",
    "                    \n",
    "                self.obj_lambda = np.clip(self.obj_lambda, a_min=1e-7, a_max=1e7)\n",
    "                \n",
    "                if reduction_ratio > 0:\n",
    "                    lossval_prev = lossval_new\n",
    "                    break\n",
    "                \n",
    "                self.session.run(self.obj_revert_op)\n",
    "                \n",
    "                #print(self.session.run([reduction_ratio, self.obj_lma._reduction_ratio_denom]))\n",
    "                \"\"\"\n",
    "                accept = True\n",
    "                if lossval_new > lossval_prev:\n",
    "                    self.obj_lambda = self.obj_lambda / self.lambda_damping\n",
    "                    self.session.run(self.obj_revert_op)\n",
    "                    accept = False\n",
    "                else:\n",
    "                    self.obj_lambda = self.obj_lambda * self.lambda_damping\n",
    "                \n",
    "                self.obj_lambda = np.clip(self.obj_lambda, a_min=1e-7, a_max=1e7)\n",
    "                if accept: \n",
    "                    lossval_prev = lossval_new\n",
    "                    break\n",
    "                \"\"\"\n",
    "            if i >= n_probe_fixed_iterations:\n",
    "                while True:\n",
    "                    self.session.run(self.probe_update_op, \n",
    "                                     feed_dict={self.probe_lambda_placeholder:self.probe_lambda})\n",
    "                    lossval_new, reduction_ratio_denom = self.session.run([self.loss_fn,\n",
    "                                                                           self.probe_lma._reduction_ratio_denom])\n",
    "                    \n",
    "                    reduction_ratio = (lossval_new - lossval_prev) / reduction_ratio_denom\n",
    "                    \n",
    "                    if reduction_ratio < 0.25:\n",
    "                        self.probe_lambda *= self.lambda_damping[0]\n",
    "                    elif reduction_ratio > 0.75:\n",
    "                        self.probe_lambda /= self.lambda_damping[1]\n",
    "                        \n",
    "                    self.probe_lambda = np.clip(self.probe_lambda, a_min=1e-7, a_max=1e7)\n",
    "                    if reduction_ratio > 0:\n",
    "                        lossval_prev = lossval_new\n",
    "                        break\n",
    "                    \n",
    "                    self.session.run(self.probe_revert_op)\n",
    "                \n",
    "                    \"\"\"\n",
    "                    accept = True\n",
    "                    if lossval_new > lossval_prev:\n",
    "                        self.probe_lambda = self.probe_lambda / self.lambda_damping\n",
    "                        self.session.run(self.probe_revert_op)\n",
    "                        accept = False\n",
    "                    else:\n",
    "                        self.probe_lambda = self.probe_lambda * self.lambda_damping\n",
    "                    self.probe_lambda = np.clip(self.probe_lambda, a_min=1e-7, a_max=1e7)\n",
    "                    if accept: \n",
    "                        lossval_prev = lossval_new\n",
    "                        break\n",
    "                    \"\"\"\n",
    "            \n",
    "            self.losses = np.append(self.losses, lossval_new)\n",
    "        if obj_clip: self.session.run(self.obj_clip_op())\n",
    "        self.obj, self.probe = self.session.run([self.tf_obj_cmplx, self.tf_probe_cmplx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
