{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import autograd as ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'autograd' from '/raid/home/skandel/code/autograd/autograd/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/code/autograd/autograd/__init__.py\n",
       "\u001b[0;31mSource:\u001b[0m     \n",
       "\u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdifferential_operators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmake_vjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultigrad_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melementwise_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgrad_and_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_tensor_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_vector_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjacobian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_jacobian_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_jacobian_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_named\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_hvp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_jvp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_ggnvp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholomorphic_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuiltins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprimitive_with_deprecation_warnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN CONSTRUCTION\n",
    "class Curveball(object):\n",
    "    \"\"\"Adapted from:\n",
    "    https://github.com/jotaf98/curveball\n",
    "    \"\"\"\n",
    "    def __init__(self, input_var, loss_fn_input, loss_fn_tensor, \n",
    "                 name,\n",
    "                 damping_factor=1.0, \n",
    "                 damping_update_factor=0.99, damping_update_frequency=1,\n",
    "                 alpha_init=1.0, squared_loss=True):\n",
    "        \"\"\"The alpha should generally just be 1.0 and doesn't change. \n",
    "        The beta and rho values are updated at each cycle, so there is no intial value.\"\"\"\n",
    "        self._name = name\n",
    "        self._input_var = input_var\n",
    "        self._loss_fn_input = loss_fn_input\n",
    "        self._loss_fn_tensor = loss_fn_tensor\n",
    "        \n",
    "        # Multiplicating factor to update the damping factor at the end of each cycle\n",
    "        self._damping_factor = damping_factor\n",
    "        self._damping_update_factor = damping_update_factor\n",
    "        self._damping_update_frequency = damping_update_frequency\n",
    "        self._squared_loss = squared_loss\n",
    "        self._alpha = alpha_init\n",
    "        \n",
    "        self._z = np.zeros_like(self._input_var)\n",
    "        self._iteration = 0\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self._damping_factor = tf.get_variable(\"lambda\", dtype=tf.float32, \n",
    "                                                  initializer=damping_factor)\n",
    "            \n",
    "            # Variable used for momentum-like updates\n",
    "            self._z = tf.get_variable(\"z\", dtype=tf.float32, \n",
    "                                     initializer=tf.zeros_like(self._input_var))#(tf.shape(self._input_var)))\n",
    "\n",
    "            self._dummy_var = tf.get_variable(\"dummy\", dtype=tf.float32, \n",
    "                                             initializer=tf.zeros_like(self._loss_fn_input))\n",
    "\n",
    "            self._loss_before_update = tf.get_variable(\"loss_before_update\", dtype=tf.float32,\n",
    "                                                     initializer=0.,\n",
    "                                                     trainable=False)\n",
    "            self._iteration = tf.get_variable(\"iteration\", shape=[], dtype=tf.int64,\n",
    "                                             initializer=tf.zeros_initializer, \n",
    "                                             trainable=False)\n",
    "            self._expected_quadratic_change = tf.get_variable(\"expected_quadratic_change\", \n",
    "                                                         dtype=tf.float32,\n",
    "                                                         initializer=0.,\n",
    "                                                         trainable=False)\n",
    "\n",
    "        # Update parameters\n",
    "\n",
    "        self.loss_after_update_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "        self._second_order()\n",
    "\n",
    "        # Update the beta and rho parameters\n",
    "        self._param_updates()\n",
    "        \n",
    "    \n",
    "    def _second_order(self):\n",
    "        self._vjp = ag.make_vjp(self.loss_fn_input)\n",
    "        self._jvpz = ag.differential_operators.make_jvp_reversemode(self.loss_fn_input)\n",
    "        \n",
    "        if self._squared_loss:\n",
    "            self._hjvpz = self._jvpz\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not implemented yet\")\n",
    "            self._hjvpz = None\n",
    "            \n",
    "            #Tensorflow version:\n",
    "            #self._hjvpz = tf.gradients(tf.gradients(self._loss_fn_tensor, \n",
    "            #                                           self._loss_fn_input)[0][None, :] \n",
    "            #                              @ self._jvpz[:,None], self._loss_fn_input,\n",
    "            #                              stop_gradients=self._jvpz)[0]\n",
    "        \n",
    "        with tf.name_scope(self._name + '_second_order'):\n",
    "            self._vjp = tf.gradients(self._loss_fn_input, self._input_var, self._dummy_var, \n",
    "                                     name='vjp')[0]\n",
    "            self._jvpz = tf.gradients(self._vjp, self._dummy_var, self._z,\n",
    "                                      name='jvpz')[0]\n",
    "\n",
    "            # this could be off by a factor of two\n",
    "            if self._squared_loss:\n",
    "                self._hjvpz = self._jvpz\n",
    "            else:\n",
    "                self._hjvpz = tf.gradients(tf.gradients(self._loss_fn_tensor, \n",
    "                                                       self._loss_fn_input)[0][None, :] \n",
    "                                          @ self._jvpz[:,None], self._loss_fn_input,\n",
    "                                          stop_gradients=self._jvpz)[0]\n",
    "\n",
    "            # Jacobian for the loss function wrt its inputs\n",
    "            self._jloss = tf.gradients(self._loss_fn_tensor, self._loss_fn_input, name='jloss')[0]\n",
    "\n",
    "            # J^T H J z\n",
    "            self._jhjvpz = tf.gradients(self._loss_fn_input, self._input_var, self._hjvpz + self._jloss, \n",
    "                                        name='jhjvpz')[0]\n",
    "            self._deltaz = self._jhjvpz + self._damping_factor * self._z    \n",
    "    \n",
    "    def _param_updates(self):\n",
    "        \n",
    "        with tf.name_scope(self._name + '_param_updates'):\n",
    "            # This is for the beta and rho updates\n",
    "            self._jvpdz = tf.gradients(self._vjp, self._dummy_var, self._deltaz, name='jvpdz')[0]\n",
    "\n",
    "            if self._squared_loss:\n",
    "                self._hjvpdz = self._jvpdz\n",
    "            else:\n",
    "                self._hjvpdz = tf.gradients(tf.gradients(self._loss_fn_tensor, \n",
    "                                                       self._loss_fn_input)[0][None, :] \n",
    "                                          @ self.jvpdz[:,None], self._loss_fn_input,\n",
    "                                          stop_gradients=self._jvpdz)[0]\n",
    "\n",
    "            a11 = tf.reduce_sum(self._hjvpdz * self._jvpdz)\n",
    "            a12 = tf.reduce_sum(self._jvpz * self._hjvpdz)\n",
    "            a22 = tf.reduce_sum(self._jvpz * self._hjvpz)\n",
    "\n",
    "            b1 = tf.reduce_sum(self._jloss * self._jvpdz)\n",
    "            b2 = tf.reduce_sum(self._jloss * self._jvpz)\n",
    "\n",
    "            a11 = a11 + tf.reduce_sum(self._deltaz * self._deltaz) * self._damping_factor\n",
    "            a12 = a12 + tf.reduce_sum(self._deltaz * self._z) * self._damping_factor\n",
    "            a22 = a22 + tf.reduce_sum(self._z * self._z) * self._damping_factor\n",
    "\n",
    "            A = tf.stack([[a11, a12],[a12, a22]])\n",
    "            b = tf.stack([b1, b2])\n",
    "\n",
    "            # Cannot use vanilla matrix inverse because the matrix is sometimes singular\n",
    "            #self.m_b = tf.reshape(tf.matrix_inverse(self.A) @ self.b[:, None], [-1])\n",
    "            \n",
    "            # Using this as a substitute for pinv\n",
    "            m_b = tf.reshape(tf.linalg.lstsq(A, b[:, None], fast=False), [-1])\n",
    "            self._beta = m_b[0]\n",
    "            self._rho = -m_b[1]\n",
    "            self._M = -0.5 * tf.reduce_sum(m_b * b)\n",
    "            \n",
    "    def damping_update(self, threshold_low=0.5, threshold_high=1.5):\n",
    "        # It turns out that tensorflow can only calculate the value of a tensor *once* during a session.run() call.\n",
    "        # This means that I cannot calculate the loss value *before* and *after* the variable update within the \n",
    "        # same session.run call. Since the damping update reuires both the values, I am separating this out.\n",
    "        \n",
    "        # Uses the placeholder \"loss_after_update\"\n",
    "        # This might be a TOO COMPLICATED way to do the damping updates.\n",
    "        \n",
    "        with tf.name_scope(self._name + '_damping_update'):\n",
    "            def update():\n",
    "                actual_loss_change = self.loss_after_update_placeholder - self._loss_before_update\n",
    "                gamma_val = actual_loss_change / self._expected_quadratic_change\n",
    "\n",
    "                f1 = lambda: tf.constant(1.0 / self._damping_update_factor)\n",
    "                f2 = lambda: tf.constant(self._damping_update_factor)\n",
    "                f3 = lambda: tf.constant(1.0)\n",
    "\n",
    "                update_factor = tf.case({tf.less(gamma_val, threshold_low):f1, \n",
    "                                 tf.greater(gamma_val, threshold_high):f2},\n",
    "                                 default=f3)\n",
    "\n",
    "                damping_factor_new = tf.clip_by_value(self._damping_factor \n",
    "                                                      * update_factor, 1e-7, 1e7)\n",
    "                return damping_factor_new\n",
    "\n",
    "            damping_new_op = lambda: tf.assign(self._damping_factor, update(), name='damping_new_op')\n",
    "            damping_same = lambda: tf.identity(self._damping_factor)\n",
    "            #damping_factor_new = tf.cond(tf.equal(self._iteration % self._damping_update_frequency, 0),\n",
    "            #                             update, damping_same)\n",
    "\n",
    "\n",
    "            #damping_update_op = tf.assign(self._damping_factor, damping_factor_new,\n",
    "            #                                    name='damping_update_op')\n",
    "            damping_update_op = tf.cond(tf.equal(self._iteration % self._damping_update_frequency, 0),\n",
    "                                        damping_new_op, damping_same)\n",
    "        return damping_update_op\n",
    "        \n",
    "    def minimize(self):\n",
    "        with tf.name_scope(self._name + '_minimize_step'):\n",
    "            counter_op = tf.assign(self._iteration, self._iteration + 1, name='counter_op')\n",
    "            quadratic_change_op = tf.assign(self._expected_quadratic_change, self._M, \n",
    "                                           name='quadratic_change_assign_op')\n",
    "            store_loss_op = tf.assign(self._loss_before_update, self._loss_fn_tensor,\n",
    "                                      name='store_loss_op')\n",
    "\n",
    "            \"\"\"Update the various variables in sequence\"\"\"\n",
    "            with tf.control_dependencies([quadratic_change_op, counter_op, store_loss_op]):\n",
    "                z_op = tf.assign(self._z, self._rho * self._z - self._beta * \n",
    "                                 self._deltaz, name='z_op')\n",
    "                \n",
    "            with tf.control_dependencies([z_op]):\n",
    "                var_update_op = tf.assign(self._input_var, self._input_var + \n",
    "                                          self._alpha * self._z, \n",
    "                                          name='var_update_op')\n",
    "        return var_update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj_guess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-46777e6e6ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_cmplx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_guess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cmplx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cmplx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvjp_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obj_guess' is not defined"
     ]
    }
   ],
   "source": [
    "x_cmplx = obj_guess.copy()\n",
    "x = np.concatenate((np.real(x_cmplx).flatten(), np.imag(x_cmplx).flatten()))\n",
    "z = np.zeros_like(x)\n",
    "\n",
    "vjp_f = ag.make_vjp(f_obj)\n",
    "jvp_f = ag.differential_operators.make_jvp_reversemode(f_obj)\n",
    "lossfn = sq_diff_f\n",
    "\n",
    "l = 1.0\n",
    "alpha = 1\n",
    "beta = 0.5\n",
    "rho = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj_guess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-46777e6e6ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_cmplx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_guess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cmplx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cmplx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvjp_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obj_guess' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "errors_cb = []\n",
    "losses_cb = []\n",
    "\n",
    "for i in tqdm(range(150)):\n",
    "    lossval_old = lossfn(x)\n",
    "    vjpfun, ans = vjp_f(x)\n",
    "    jvpfun = jvp_f(x)\n",
    "\n",
    "    jz = jvpfun(z)\n",
    "    \n",
    "    diffvals = -diff_f_obj(x)\n",
    "    jl = vjpfun(diffvals)\n",
    "    \n",
    "    hessian_term = vjpfun(jz + diffvals)\n",
    "    delta_z = hessian_term + l * z\n",
    "    \n",
    "    jdz = jvpfun(delta_z)\n",
    "    \n",
    "    a11 = np.dot(jdz, jdz)\n",
    "    a12 = np.dot(jz, jdz)\n",
    "    a22 = np.dot(jz, jz)\n",
    "    \n",
    "    b1 = np.dot(diffvals, jdz)\n",
    "    b2 = np.dot(diffvals, jz)\n",
    "    \n",
    "    a11 = a11 + np.dot(delta_z, delta_z) * l\n",
    "    a12 = a12 + np.dot(delta_z, z) * l\n",
    "    a22 = a22 + np.dot(z, z) * l\n",
    "    \n",
    "    A = np.array([[a11, a12],[a12, a22]])\n",
    "    b = np.array([b1, b2])\n",
    "    m_b = np.linalg.pinv(A) @ b\n",
    "    beta = m_b[0]\n",
    "    rho = -m_b[1]\n",
    "    \n",
    "    z = rho * z - beta * delta_z\n",
    "    x = x + alpha * z\n",
    "    \n",
    "    M = -0.5 * np.dot(m_b, b)\n",
    "    lossval_new = lossfn(x)\n",
    "     \n",
    "    gamma = (lossval_new - lossval_old) / M\n",
    "\n",
    "    if gamma < 0.5:\n",
    "        l = l / 0.9\n",
    "    elif gamma > 1.5:\n",
    "        l = l * 0.9\n",
    "    l = np.clip(l, 1e-7, 1e7)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        size = x.size // 2\n",
    "        x_reshaped = np.reshape(x[:size] + 1j * x[size:], obj_true.shape)\n",
    "        roll, err, phase = register_translation(x_reshaped, obj_true, 10)\n",
    "        roll, err, phase = register_translation(x_reshaped * np.exp(-1j * phase), obj_true, 10)\n",
    "        errors_cb.append(err)\n",
    "        losses_cb.append(lossval_new)\n",
    "        print(f'{i} {lossval_old:3.5g} {lossval_new:3.5g} {beta:3.5g}  {rho:3.5g} {gamma:3.5g} {l:3.5g} {err:3.5g}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
