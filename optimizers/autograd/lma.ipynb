{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import autograd as ag\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import scipy\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMA(object):\n",
    "    \"\"\"The Levenberg-Marquardt algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_variable: np.ndarray,\n",
    "                 loss_input_fn: Callable[[np.ndarray], np.ndarray],\n",
    "                 loss_fn: Callable[[np.ndarray], float],\n",
    "                 damping_factor: float = 1.0, \n",
    "                 damping_update_factor: float = 2/3,\n",
    "                 update_cond_threshold_low: float = 0.25, # following the least squares book\n",
    "                 update_cond_threshold_high: float = 0.75,\n",
    "                 damping_threshold_low = 1e-7,\n",
    "                 damping_threshold_high = 1e7,\n",
    "                 max_cg_iter: int = 100,\n",
    "                 cg_tol: float = 1e-5,\n",
    "                 squared_loss: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Paramters:\n",
    "        input_var: \n",
    "        1-d numpy array. Flatten before passing, if necessary.\n",
    "        \n",
    "        loss_input_fn: \n",
    "        Function that takes in input_var as the only input parameter.\n",
    "        This should output a 1-d array, that is then passed to loss_fn for the \n",
    "        actual loss calculation.  \n",
    "        Separating the loss calculation into a two step process this way \n",
    "        simplifies the second order calculations.\n",
    "        \n",
    "        loss_fn:\n",
    "        Function that takes in the output of loss_input_fn and \n",
    "        calculates the singular loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._input_var = input_variable\n",
    "        self._loss_input_fn = loss_input_fn\n",
    "        self._loss_fn = loss_fn\n",
    "        \n",
    "        # Multiplicating factor to update the damping factor at the end of each cycle\n",
    "        self._damping_factor = damping_factor\n",
    "        self._damping_update_factor = damping_update_factor\n",
    "        self._update_cond_threshold_low = update_cond_threshold_low\n",
    "        self._update_cond_threshold_high =  update_cond_threshold_high\n",
    "        self._damping_threshold_low = damping_threshold_low\n",
    "        self._damping_threshold_high = damping_threshold_high\n",
    "        self._max_cg_iter = max_cg_iter\n",
    "        self._cg_tol = cg_tol\n",
    "        self._squared_loss = squared_loss\n",
    "        \n",
    "        # variable used for the updates\n",
    "        self._update_var = np.zeros_like(self._input_var)\n",
    "        \n",
    "        self._vjp = ag.make_vjp(self._loss_input_fn)\n",
    "        self._jvp = ag.differential_operators.make_jvp_reversemode(self._loss_input_fn)\n",
    "        \n",
    "        self._grad = ag.grad(self._loss_fn)\n",
    "        \n",
    "        if self._squared_loss:\n",
    "            self._hjvp = self._jvp\n",
    "        else:\n",
    "            self._hjvp = ag.differential_operators.make_hvp(self._loss_fn)\n",
    "\n",
    "    def _matrix_vector_operators(self) -> List[float]:   \n",
    "        vjp_fun_this, predictions_array = self._vjp(self._input_var) \n",
    "        jvp_fun_this = self._jvp(self._input_var)\n",
    "        \n",
    "        loss_before_update = self._loss_fn(predictions_array)\n",
    "        \n",
    "        if self._squared_loss: \n",
    "            hjvp_fun_this = lambda x: x#jvp_fun_this\n",
    "            jloss = self._grad(predictions_array)\n",
    "        else:\n",
    "            hjvp_fun_this, jloss = self._hjvp(predictions_array)\n",
    "        \n",
    "        return vjp_fun_this, hjvp_fun_this, jloss, loss_before_update\n",
    "    \n",
    "    def minimize(self):\n",
    "        \n",
    "        [\n",
    "            vjp_fun_this, \n",
    "            hjvp_fun_this,\n",
    "            jloss, \n",
    "            loss_before_update \n",
    "        ] = self._matrix_vector_operators()\n",
    "        \n",
    "        linear_b = vjp_fun_this(jloss)\n",
    "        \n",
    "        while True:\n",
    "            linear_ax = lambda h: vjp_fun_this(hjvp_fun_this(h)) + self._damping_factor * h\n",
    "            \n",
    "            # I am planning on trying out both the scipy linear solver \n",
    "            # and my own conjugate gradient solver.\n",
    "            # For the initial guess, I am following Marten's recipe\n",
    "            # i.e., using the solution from the previous run.\n",
    "            \n",
    "            A = LinearOperator((linear_b.size, linear_b.size), matvec=linear_ax)\n",
    "            opt_out = scipy.sparse.linalg.cg(A, linear_b, tol=self._cg_tol, \n",
    "                                             x0=self._update_var,\n",
    "                                             maxiter=self._max_cg_iter)\n",
    "            if opt_out[1] < 0:\n",
    "                raise ValueError(\"Linear system not correctly solved\")\n",
    "            \n",
    "            update_this = opt_out[0]\n",
    "            x_new = self._input_var + update_this\n",
    "            \n",
    "            loss_new = self._loss_fn(self._predictions_fn(x_new))\n",
    "            loss_change = loss_new - loss_before_update\n",
    "            expected_quadratic_change = np.dot(update_this, self._damping_factor * update_this + linear_b)\n",
    "            reduction_ratio = loss_change / expected_quadratic_change\n",
    "            \n",
    "            if reduction_ratio > self._update_cond_threshold_high:\n",
    "                self._damping_factor *= self._damping_update_factor\n",
    "            elif reduction_ratio < self._update_cond_threshold_low:\n",
    "                self._damping_factor *= 1 / self._damping_update_factor \n",
    "            \n",
    "            if reduction_ratio > 0:\n",
    "                self._update_var = update_this\n",
    "                self._input_var = x_new\n",
    "                break      \n",
    "        return self._input_var\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
